# Marvin-Azuogu-AI-Portfolio-

├── README.md: This the overview of the skills and technologies I learned in ITAI 2373 Natural Language Processing (NLP) class, which I gained a practical understanding of the following fundamental NLP processes and techniques:
NLP Processing Techniques: You learned the foundational steps of processing raw text data. This includes tasks like tokenization (breaking text into words or sentences), normalization, and stemming or lemmatization (reducing words to their base form).

Text Representation: I learned how to transform unstructured text into a numerical format that machine learning models can understand. This likely involved techniques such as Bag-of-Words, TF-IDF (Term Frequency-Inverse Document Frequency), or word embeddings.

Part-of-Speech (POS) Tagging: I practiced identifying and tagging words in a text based on their grammatical role (e.g., noun, verb, adjective), a key step in many NLP pipelines.

Sentiment and Emotion Analysis: I learned how to analyze text to determine the underlying sentiment (e.g., positive, negative, neutral) or specific emotions (e.g., joy, anger, sadness). This is a crucial skill for understanding public opinion and customer feedback.

Project-Based Skills
The curriculum was heavily focused on applying skills through practical, hands-on projects.

Creative Challenges: A project like "Voice Tech in the Multiverse" involved exploring how voice-based technology and NLP can be used in creative or unconventional ways. This taught me to think outside the box and apply my skills to novel problems.

NewsBot Intelligence System: This mid-term and final project was a comprehensive application of my skills. I built a system to process news articles, extract information, analyze sentiment, or summarize content. This capstone project demonstrated the ability to integrate various NLP techniques into a functional, real-world application.

Key Technologies and Tools
The course provided hands-on experience with industry-standard tools and platforms.

Jupyter: I used Jupyter notebooks as my primary environment for writing and executing code, allowing me to combine live code, visualizations, and explanatory text in a single document.

GitHub: I used Git and GitHub for version control and collaborative work on projects, which are essential skills for any developer or data scientist.

Python Libraries: While not explicitly listed, the course topics strongly imply the use of popular NLP libraries such as NLTK, spaCy, or Hugging Face Transformers for tasks like text preprocessing, POS tagging, and sentiment analysis.

│   ├── Project1/NPL in Action- From Pop Culture to Processes-Marvin Azuogu

│   ├── README.md: J.A.R.V.I.S. from Iron Man is a prime example of advanced conversational AI, capable of understanding complex spoken commands, engaging in natural dialogue, and providing real-time data analysis. Similarly, Star Trek’s Universal Translator demonstrates real-time machine translation, instantly removing language barriers.

How J.A.R.V.I.S. Works (NLP Analysis):

Natural Language Understanding (NLU): Comprehends spoken language, including slang and informal tone, to accurately interpret intent.

Information Retrieval & Knowledge Extraction: Quickly searches vast structured and unstructured data to find relevant information.

Natural Language Generation (NLG): Produces clear, context-aware, human-like responses.
These systems showcase the integration of multiple NLP tasks to enable seamless human-computer interaction.

│   │   ├── model.py/...

│   │   ├── dataset/https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/blob/193f3d3acb3c8b4b3912c0f618821f99fcd4752f/NPL%20in%20Action-%20From%20Pop%20Culture%20to%20Processes-Marvin%20Azuogu.pdf

│   │   └── results/My results was the connection between the fundamental skills learned in an NLP preprocessing lab and the advanced AI seen in pop culture. The lab taught you that preprocessing is a crucial step for managing the complexity of human language. This foundation enables more sophisticated NLP tasks, such as the Natural Language Understanding (NLU), Information Retrieval, and Natural Language Generation (NLG) required for a system like J.A.R.V.I.S. (from Iron Man) to function effectively. Ultimately, the document shows how these core concepts are integrated to create seamless human-computer interactions, as also exemplified by the Universal Translator from Star Trek.

│   └── Project2/L02 - NLP Processing Techniques

│   ├── README.md This lab was a transformative experience that highlighted the critical role of preprocessing in NLP. You realized that preprocessing is not just a preliminary step, but a strategic process for managing the "messiness" and complexity of human language.

A key insight was the comparison between NLTK and spaCy for tokenization, where you discovered spaCy's integrated pipeline offers a more holistic and production-ready approach by providing POS tags and lemmas simultaneously. I also gained a deeper understanding of the trade-offs involved in stop word removal and the distinct differences between stemming and lemmatization. For tasks requiring high accuracy and meaning preservation, such as sentiment analysis, you concluded that lemmatization is the superior choice, while stemming is a faster, though less precise, alternative.

Finally, I learned that an effective preprocessing strategy must be adaptive, tailored to the specific characteristics of the text data, and that this understanding is crucial for making practical decisions in future NLP projects.

│   │   ├── model.py/https:/https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/blob/9d96053975b558d95f4a281699857d054e4d9a17/L02_Marvin_Azuogu_ITAI_2373.ipynb

│   │   ├── dataset/https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/blob/d24bbcb8f2e4abc5bbbea6781942788fd4b051c4/L02_Journal_Marvin%20Azuogu_ITAI_2373.pdf

│   │   └── results/The results of my NLP Processing Techniques is a pivotal, strategic phase in NLP, not a mere preliminary step. I gained key insights into the "messiness" of human language and the critical trade-offs involved. I learned that spaCy's integrated approach to tokenization is more holistic than NLTK's, and that lemmatization is often a better choice than stemming for tasks requiring high accuracy, such as sentiment analysis. Ultimately, the lab taught me that the best preprocessing pipeline is an adaptive one, carefully tailored to the specific type of data and the goals of the project.


│   ├── Project3/Intro to Jupyter and GitHub/

│   ├── README.md In this lab, I successfully completed my first exploration of Jupyter Notebooks and GitHub, learning how to use both for interactive computing and version control.

Jupyter Notebook Experience: I accessed Jupyter through Anaconda Navigator and created a new notebook. I practiced using both markdown and code cells, performing basic Python operations and documenting your work with narrative explanations.

GitHub Experience: I created a new public repository and uploaded your notebook file, gaining a foundational understanding of how GitHub functions as a platform for remote storage and collaboration.

Key Learnings
Jupyter Notebook: I learned the key difference between markdown and code cells and found value in being able to mix code with narrative explanations for clear documentation.

GitHub: I gained insight into how developers manage code versions and collaborate on projects by creating a repository and managing file visibility.

Challenges and Resolutions: I successfully navigated and overcame two key challenges: a file upload issue on GitHub and initially understanding the distinction between markdown and code cells.

Next Steps
My experience has sparked an interest in exploring more direct integrations between Jupyter and GitHub, such as using the Git command-line interface or extensions like JupyterLab-Git, which would streamline the process of version control for my data science projects.

│   │   ├── model.py/

│   │   ├── dataset/https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/blob/407de435f2e1c20eba1d69602b5ac69663b04882/L001_Marvin_Azuogu_ITAI2373.pdf

│   │   └── results/I successfully learned to use Jupyter Notebooks for interactive computing and GitHub for version control. I discovered the value of mixing markdown and code in Jupyter for clear documentation and gained a foundational understanding of how GitHub facilitates collaboration. I equally overcame initial challenges with file uploads and are now interested in exploring more direct integrations between the two platforms.

│   ├── Project4/A03 Voice Tech in the Multiverse Creative Challenge

│   ├── README.md The A03: Voice Tech in the Multiverse Creative Challenge is an innovative assignment that requires you to act as a Chief Audio Engineer for Multiverse Entertainment Studios. The goal is to develop a cutting-edge voice technology solution for a fictional universe, showcasing a deep understanding of audio processing, acoustic challenges, and ethical considerations. The assignment fosters technical skills, problem-solving, and imaginative storytelling. 
The "A03: Voice Tech in the Multiverse Creative Challenge," where we designed a Universal Translator called "ZyraLink" for an alien species, the Zy'reel, in the kingdom of Zylaris. The challenge required a technical solution to address the unique problems of the universe, including ultrasonic and bioluminescent communication, a chaotic environment with electromagnetic storms and variable gravity, and non-human vocal anatomy. The solution, ZyraLink, uses a custom preprocessing pipeline with specialized hardware and software to capture, normalize, and align multimodal signals before a neural translation model converts them into human-readable language.

│   │   ├── model.py/

│   │   ├── dataset/[https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/blob/653b924c1b97dbc0d7098a2c599238fdd6933c79/A03_FSM%C2%B2%20(FSM%20Squared)_FaizaAbdullah_ITA2373%20(1).pdf
](https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/tree/8432859633baad51dc9320472d3f468c21cf0b01/A03%20Voice%20Tech%20in%20the%20Multiverse%20Creative%20Challenge)
│   │   └── results/It was a creative challenge where you designed a universal translator named "ZyraLink" for an alien species, the Zy'reel. There was unique acoustic details, environmental, and biological challenges of the alien world, Zylaris, and equally technical solutions. These solutions include a custom preprocessing pipeline with specialized hardware to capture both ultrasonic and bioluminescent signals, and software to filter noise, normalize signals, align multimodal data, and translate the communication using a neural network model.

│   ├── Project5/Lab 04: Text Representation
│   ├── README.md 

│   │   ├── model.py/https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/blob/063eca5f0198c91f9475b82395e75085c7efe241/L04__Marvin%20Azuogu_ITAI_I2373%20(1).ipynb


│   │   ├── dataset/https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/blob/b83f0f3825e505071cd76fd90b4e5cf2875f8a2b/L04_Journal_Marvin_Azuogu_ITAI_2373.docx

│   │   └── results/

│   ├── Project6/Lab 05: Part-of-Speech Tagging in the Real World

│   ├── README.md 

│   │   ├── model.py/https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/blob/8420f6866cf061f9bbcd805d23762c3697c7bfef/L05_Azuogu_Marvin_ITAI2373.ipynb

│   │   ├── dataset/

│   │   └── results/

│   ├── Project7/L07 Sentiment and Emotion Analysis in the Real World

│   ├── README.md 

│   │   ├── model.py/https://github.com/VinbelKing/Marvin-Azuogu-AI-Portfolio-/blob/57abd8ef5e9184ccac81ba4f04ec16f2efdaab9e/L07_Azuogu_Marvin_ITAI2373%20(1).ipynb

│   │   ├── dataset/

│   │   └── results/

│   ├── Project8/Mid-Term Group Project: NewsBot Intelligence System

│   ├── README.md 

│   │   ├── model.py/https://github.com/VinbelKing/ITAI-2373-Portfolio-Marvin.git

│   │   ├── dataset/

│   │   └── results/

│   ├── Project9/Final Project: NewsBot Intelligence System 2.0

│   ├── README.md 

│   │   ├── model.py/

│   │   ├── dataset/

│   │   └── results/

Marvin Azuogu
marvin.azuougu@gmail.com




